{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "id": "21af0347",
      "cell_type": "markdown",
      "source": "# E\u2011ConvNeXt Image Classification Pipeline\n\nThis notebook presents a unified image classification workflow built around the E\u2011ConvNeXt architecture. It brings together several pieces of functionality from separate notebooks and scripts into one cohesive pipeline:\n\n* **Automated data splitting:** given a root directory of images, the notebook can automatically split the data into training, validation and test sets (70/15/15 split by default).\n* **Exploratory Data Analysis (EDA):** before training, the notebook scans the dataset and visualises key characteristics such as class distribution, image dimensions, aspect ratios and brightness. Example images are also shown.\n* **E\u2011ConvNeXt model definition and training:** the core model, training loop and evaluation functions are adapted from the *E\u2011ConvNeXt Notebook Guide\u2011FULLPIPELINE* to provide a configurable classification backbone. Training history is plotted using a consistent blue\u2011gradient colour palette.\n\nTo use this notebook with your own data, set the `dataset_root` variable in the next code cell to point to your image folder. The folder can either contain class sub\u2011directories directly or be organised into `train`, `val`/`validation` and `test` folders. Once the dataset is split and loaded, the notebook will run EDA, train the E\u2011ConvNeXt model and provide evaluation metrics and visualisations.",
      "metadata": {}
    },
    {
      "id": "9ad48ab8",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# Install required libraries (uncomment if running locally)\n# !pip install torch torchvision timm numpy pandas matplotlib seaborn scikit-learn\n\nimport os\nimport shutil\nimport random\nimport math\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nfrom torchvision import datasets, transforms\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Set random seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# Use a blue gradient palette for all plots\nsns.set_palette('Blues')",
      "outputs": []
    },
    {
      "id": "97c8c659",
      "cell_type": "markdown",
      "source": "## Automated dataset splitting\n\nThe following function inspects a dataset directory and organises it into `train`, `val` and `test` sub\u2011folders on disk.\n\n* If the root contains only class sub\u2011folders, a 70/15/15 split is created.\n* If only `train` and `val` directories exist, their contents are combined and re\u2011split into `train`, `val` and `test`.\n* If `train` and `test` directories exist, they are left untouched (no validation split).\n* If `train`, `val` and `test` are present, no splitting is performed.\n\nYou can adjust the `train_ratio`, `val_ratio` and `test_ratio` arguments to change the default 70/15/15 proportions.",
      "metadata": {}
    },
    {
      "id": "34d2da44",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def split_dataset(base_dir: str, train_ratio: float = 0.70, val_ratio: float = 0.15, test_ratio: float = 0.15, seed: int = 42) -> None:\n    \"\"\"\n    Split a dataset of images on disk. The function operates in four modes based on folder structure:\n\n    1. When the root contains only class sub\u2011folders (no `train`/`val`/`test`), a new `train`/`val`/`test` split is created.\n    2. When `train` and `val` folders exist, their contents are combined and re\u2011split into `train`/`val`/`test`.\n    3. When `train` and `test` exist, they are left untouched and no validation set is created.\n    4. When `train`, `val` and `test` exist, the directory structure is assumed correct and nothing is changed.\n\n    Parameters\n    ----------\n    base_dir : str\n        Path to the dataset root.\n    train_ratio : float\n        Proportion of samples to allocate to the training set when performing a new split.\n    val_ratio : float\n        Proportion of samples to allocate to the validation set when performing a new split.\n    test_ratio : float\n        Proportion of samples to allocate to the test set when performing a new split.\n    seed : int\n        Random seed for reproducibility.\n    \"\"\"\n    random.seed(seed)\n    root = os.path.abspath(base_dir)\n    train_dir = os.path.join(root, 'train')\n    val_dir = os.path.join(root, 'val')\n    test_dir = os.path.join(root, 'test')\n\n    def _ensure_empty(dir_path: str) -> None:\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n        else:\n            for item in os.listdir(dir_path):\n                shutil.rmtree(os.path.join(dir_path, item))\n\n    has_train = os.path.isdir(train_dir)\n    has_val = os.path.isdir(val_dir)\n    has_test = os.path.isdir(test_dir)\n\n    # Case 1: single folder of class sub\u2011directories\n    if not has_train and not has_val and not has_test:\n        print('No train/val/test directories detected. Creating a 70/15/15 split.')\n        classes = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n        _ensure_empty(train_dir)\n        _ensure_empty(val_dir)\n        _ensure_empty(test_dir)\n        for cls in classes:\n            src_dir = os.path.join(root, cls)\n            images = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n            random.shuffle(images)\n            n_total = len(images)\n            n_train = int(n_total * train_ratio)\n            n_val = int(n_total * val_ratio)\n            splits = {\n                'train': images[:n_train],\n                'val': images[n_train:n_train + n_val],\n                'test': images[n_train + n_val:]\n            }\n            for split_name, files in splits.items():\n                target_dir = os.path.join(root, split_name, cls)\n                os.makedirs(target_dir, exist_ok=True)\n                for fname in files:\n                    shutil.copy2(os.path.join(src_dir, fname), os.path.join(target_dir, fname))\n    # Case 2: train and val only \u2192 combine and re\u2011split\n    elif has_train and has_val and not has_test:\n        print('Found train and val directories only. Combining and re\u2011splitting into train/val/test.')\n        combined = {}\n        for split_dir in [train_dir, val_dir]:\n            for cls in os.listdir(split_dir):\n                cls_dir = os.path.join(split_dir, cls)\n                combined.setdefault(cls, []).extend([\n                    os.path.join(cls_dir, f) for f in os.listdir(cls_dir)\n                    if os.path.isfile(os.path.join(cls_dir, f))\n                ])\n        _ensure_empty(train_dir)\n        _ensure_empty(val_dir)\n        _ensure_empty(test_dir)\n        for cls, files in combined.items():\n            random.shuffle(files)\n            n_total = len(files)\n            n_train = int(n_total * train_ratio)\n            n_val = int(n_total * val_ratio)\n            splits = {\n                'train': files[:n_train],\n                'val': files[n_train:n_train + n_val],\n                'test': files[n_train + n_val:]\n            }\n            for split_name, filepaths in splits.items():\n                target_dir = os.path.join(root, split_name, cls)\n                os.makedirs(target_dir, exist_ok=True)\n                for src_path in filepaths:\n                    shutil.copy2(src_path, os.path.join(target_dir, os.path.basename(src_path)))\n    # Case 3: train and test only \u2192 do nothing\n    elif has_train and not has_val and has_test:\n        print('Found train and test directories only. Leaving as\u2011is. No validation set will be created.')\n    # Case 4: train, val and test exist \u2192 do nothing\n    elif has_train and has_val and has_test:\n        print('Found train, val, and test directories. No splitting needed.')\n    else:\n        raise RuntimeError(f'Unrecognised layout under {root}. Expected combinations of train/val/test.')\n",
      "outputs": []
    },
    {
      "id": "74ededdf",
      "cell_type": "markdown",
      "source": "### Configuration and data loading\n\nThe configuration below allows you to customise the dataset location, image resolution, model variant and training hyperparameters. After splitting the dataset, the `load_data` function loads the images into `torchvision.datasets.ImageFolder` datasets and constructs PyTorch dataloaders for the training and validation sets.",
      "metadata": {}
    },
    {
      "id": "e0a738fb",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# Editable configuration\n@dataclass\nclass Config:\n    DATA_DIR: str = 'path/to/your/dataset'  # change this to your dataset root\n    NUM_CLASSES: int = 10\n    MODEL_VARIANT: str = 'tiny'            # choose from 'mini', 'tiny', 'small'\n    IMAGE_SIZE: int = 224\n    BATCH_SIZE: int = 32\n    EPOCHS: int = 10\n    LEARNING_RATE: float = 2.5e-4\n    WEIGHT_DECAY: float = 0.05\n    # augmentation settings\n    RAND_AUGMENT_N: int = 9\n    RAND_AUGMENT_M: float = 0.5\n    MIXUP_ALPHA: float = None\n    CUTMIX_ALPHA: float = None\n\nconfig = Config()\n\ndef get_transforms(cfg: Config):\n    train_transforms = transforms.Compose([\n        transforms.Resize(cfg.IMAGE_SIZE + 32),\n        transforms.RandomCrop(cfg.IMAGE_SIZE),\n        transforms.RandAugment(num_ops=cfg.RAND_AUGMENT_N, magnitude=int(cfg.RAND_AUGMENT_M * 10)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    val_transforms = transforms.Compose([\n        transforms.Resize(cfg.IMAGE_SIZE + 32),\n        transforms.CenterCrop(cfg.IMAGE_SIZE),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    return train_transforms, val_transforms\n\ndef load_data(cfg: Config):\n    train_transforms, val_transforms = get_transforms(cfg)\n    train_dir = os.path.join(cfg.DATA_DIR, 'train')\n    val_dir = os.path.join(cfg.DATA_DIR, 'val')\n    train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n    val_dataset = datasets.ImageFolder(val_dir, transform=val_transforms)\n    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n    return train_loader, val_loader, train_dataset.classes\n",
      "outputs": []
    },
    {
      "id": "8b873275",
      "cell_type": "markdown",
      "source": "## Exploratory Data Analysis (EDA)\n\nThe following helpers perform a lightweight scan of an image dataset and produce visual summaries. Statistics such as image width, height, aspect ratio, file size, brightness and colourfulness are collected. Plotting functions then visualise class distribution, dimension distributions, aspect ratios and other metrics. All charts use the same blue\u2011gradient palette for consistency.",
      "metadata": {}
    },
    {
      "id": "3ed8cfc3",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from PIL import Image\n\ndef _colorfulness_fast(im: Image.Image) -> float:\n    arr = np.asarray(im.convert('RGB'), dtype=np.float32)\n    r, g, b = arr[..., 0], arr[..., 1], arr[..., 2]\n    rg = np.abs(r - g)\n    yb = np.abs(0.5 * (r + g) - b)\n    return float(math.sqrt(rg.var() + yb.var()) + 0.3 * math.sqrt(rg.mean()**2 + yb.mean()**2))\n\ndef _brightness(im: Image.Image) -> float:\n    return float(np.asarray(im.convert('L'), dtype=np.float32).mean())\n\ndef scan_image_dataset(data_path: str) -> pd.DataFrame:\n    \"\"\"\n    Perform a scan over all images under `data_path` and return a DataFrame summarising their properties.\n\n    The scan looks recursively for files with typical image extensions. For each image it records\n    the label (sub\u2011directory name), optional split (train/val/test if present), dimensions, aspect ratio,\n    file size (kilobytes), brightness and colourfulness.\n    \"\"\"\n    img_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}\n    records = []\n    for root_dir, _, files in os.walk(data_path):\n        for fname in files:\n            if not any(fname.lower().endswith(ext) for ext in img_exts):\n                continue\n            fpath = os.path.join(root_dir, fname)\n            try:\n                with Image.open(fpath) as im:\n                    w, h = im.size\n                    mode = im.mode\n                    brightness_value = _brightness(im)\n                    colorfulness_value = _colorfulness_fast(im)\n            except Exception:\n                continue\n            parts = os.path.normpath(fpath).split(os.sep)\n            label = parts[-2] if len(parts) >= 2 else ''\n            split = ''\n            if label.lower() in {'train', 'val', 'validation', 'test'}:\n                split = label.lower()\n                label = parts[-3] if len(parts) >= 3 else label\n            records.append({\n                'path': fpath,\n                'split': split,\n                'label': label,\n                'width': w,\n                'height': h,\n                'aspect': (w / h) if h > 0 else float('nan'),\n                'file_size_kb': os.path.getsize(fpath) / 1024.0,\n                'brightness': brightness_value,\n                'colorfulness': colorfulness_value,\n                'mode': mode\n            })\n    return pd.DataFrame(records)\n\ndef plot_class_distribution(df: pd.DataFrame) -> None:\n    counts = df['label'].value_counts().sort_values(ascending=False)\n    plt.figure(figsize=(max(6, 0.35 * len(counts) + 4), 4))\n    sns.barplot(x=counts.index, y=counts.values)\n    plt.xticks(rotation=45, ha='right')\n    plt.title('Class Distribution')\n    plt.xlabel('Class')\n    plt.ylabel('Number of images')\n    plt.show()\n\ndef plot_histogram(series: np.ndarray, bins: int, title: str, xlabel: str) -> None:\n    plt.figure(figsize=(6, 4))\n    plt.hist(series, bins=bins, color=sns.color_palette('Blues')[3])\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel('Count')\n    plt.show()\n\ndef plot_scatter(x: np.ndarray, y: np.ndarray, title: str, xlabel: str, ylabel: str) -> None:\n    plt.figure(figsize=(5, 5))\n    plt.scatter(x, y, alpha=0.6, color=sns.color_palette('Blues')[4])\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.show()\n\ndef show_random_images(df: pd.DataFrame, n: int = 9) -> None:\n    n = min(n, len(df))\n    indices = np.random.choice(len(df), size=n, replace=False)\n    sample_paths = df.iloc[indices]['path'].tolist()\n    grid_size = int(math.sqrt(n))\n    fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size * 3, grid_size * 3))\n    for idx, path in enumerate(sample_paths):\n        row = idx // grid_size\n        col = idx % grid_size\n        ax = axes[row, col]\n        try:\n            img = Image.open(path).convert('RGB')\n            ax.imshow(img)\n            ax.set_title(os.path.basename(os.path.dirname(path)), fontsize=8)\n        except Exception:\n            ax.axis('off')\n            continue\n        ax.axis('off')\n    plt.suptitle('Random Image Samples')\n    plt.tight_layout()\n    plt.show()\n\ndef run_eda(data_path: str) -> pd.DataFrame:\n    df = scan_image_dataset(data_path)\n    if df.empty:\n        print(f'No images found under: {data_path}')\n        return df\n    plot_class_distribution(df)\n    plot_histogram(df['width'].values, bins=40, title='Image Width Distribution', xlabel='Width (pixels)')\n    plot_histogram(df['height'].values, bins=40, title='Image Height Distribution', xlabel='Height (pixels)')\n    aspects = df['aspect'].dropna().values\n    plot_histogram(aspects, bins=40, title='Aspect Ratio Distribution', xlabel='Aspect ratio (W/H)')\n    plot_histogram(df['file_size_kb'].values, bins=40, title='File Size Distribution', xlabel='File size (KB)')\n    plot_histogram(df['brightness'].values, bins=40, title='Brightness Distribution', xlabel='Average grayscale value (0\u2013255)')\n    plot_scatter(df['width'].values, df['height'].values, title='Width vs Height', xlabel='Width (pixels)', ylabel='Height (pixels)')\n    show_random_images(df, n=9)\n    return df\n",
      "outputs": []
    },
    {
      "id": "5e2fd994",
      "cell_type": "markdown",
      "source": "## E\u2011ConvNeXt model architecture\n\nThe following cells define the E\u2011ConvNeXt architecture adapted from the reference implementation.\nIt comprises an Effective Squeeze\u2011and\u2011Excitation (ESE) block, a modified ConvNeXt block with Batch Normalisation, a Cross\u2011Stage Partial (CSP) stage and the overall model class.",
      "metadata": {}
    },
    {
      "id": "e148211c",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# Effective Squeeze\u2011and\u2011Excitation block\nclass ESEBlock(nn.Module):\n    def __init__(self, channels: int, reduction: int = 4):\n        super().__init__()\n        reduced_channels = max(1, channels // reduction)\n        self.squeeze = nn.AdaptiveAvgPool2d(1)\n        self.excitation = nn.Sequential(\n            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=True),\n            nn.GELU(),\n            nn.Conv2d(reduced_channels, channels, kernel_size=1, bias=True),\n            nn.Sigmoid()\n        )\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        s = self.squeeze(x)\n        e = self.excitation(s)\n        return x * e\n\n# E\u2011ConvNeXt block\nclass EConvNeXtBlock(nn.Module):\n    def __init__(self, dim: int, expansion_factor: float = 4.0):\n        super().__init__()\n        expanded_dim = int(dim * expansion_factor)\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)\n        self.bn1 = nn.BatchNorm2d(dim)\n        self.pwconv1 = nn.Conv2d(dim, expanded_dim, kernel_size=1)\n        self.bn2 = nn.BatchNorm2d(expanded_dim)\n        self.act = nn.GELU()\n        self.pwconv2 = nn.Conv2d(expanded_dim, dim, kernel_size=1)\n        self.bn3 = nn.BatchNorm2d(dim)\n        self.ese = ESEBlock(dim)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        residual = x\n        x = self.dwconv(x)\n        x = self.bn1(x)\n        x = self.pwconv1(x)\n        x = self.bn2(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        x = self.bn3(x)\n        x = self.ese(x)\n        return x + residual\n\n# CSP stage\nclass CSPStage(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, num_blocks: int, ch_mid: int = None):\n        super().__init__()\n        self.downsample = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=2, stride=2),\n            nn.BatchNorm2d(out_channels),\n            nn.GELU()\n        )\n        if ch_mid is None:\n            ch_mid = (in_channels + out_channels) // 2\n        self.split_conv0 = nn.Sequential(\n            nn.Conv2d(out_channels, ch_mid, kernel_size=1),\n            nn.BatchNorm2d(ch_mid),\n            nn.GELU()\n        )\n        self.split_conv1 = nn.Sequential(\n            nn.Conv2d(out_channels, ch_mid, kernel_size=1),\n            nn.BatchNorm2d(ch_mid),\n            nn.GELU()\n        )\n        self.blocks = nn.Sequential(*[EConvNeXtBlock(ch_mid) for _ in range(num_blocks)])\n        self.after_blocks = nn.Sequential(\n            nn.Conv2d(ch_mid, ch_mid, kernel_size=1),\n            nn.BatchNorm2d(ch_mid),\n            nn.GELU()\n        )\n        self.merge_conv = nn.Sequential(\n            nn.Conv2d(ch_mid * 2, out_channels, kernel_size=1),\n            nn.BatchNorm2d(out_channels),\n            nn.GELU()\n        )\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.downsample(x)\n        y1 = self.split_conv0(x)\n        y2 = self.split_conv1(x)\n        y2 = self.blocks(y2)\n        y2 = self.after_blocks(y2)\n        merged = torch.cat([y1, y2], dim=1)\n        return self.merge_conv(merged)\n\ndef get_variant_config(variant: str):\n    variant = variant.lower()\n    if variant == 'mini':\n        dims = [48, 96, 192, 384]\n        depths = [3, 3, 9, 3]\n    elif variant == 'tiny':\n        dims = [64, 128, 256, 512]\n        depths = [3, 3, 9, 3]\n    elif variant == 'small':\n        dims = [80, 160, 320, 640]\n        depths = [3, 3, 27, 3]\n    else:\n        raise ValueError(f'Unknown variant: {variant}')\n    return dims, depths\n\nclass EConvNeXt(nn.Module):\n    def __init__(self, num_classes: int = 1000, variant: str = 'tiny'):\n        super().__init__()\n        dims, depths = get_variant_config(variant)\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=2, stride=2),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, dims[0], kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(dims[0]),\n            nn.GELU(),\n        )\n        in_channels = dims[0]\n        stages = []\n        for out_channels, depth in zip(dims, depths):\n            stages.append(CSPStage(in_channels, out_channels, num_blocks=depth))\n            in_channels = out_channels\n        self.stages = nn.Sequential(*stages)\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.head = nn.Linear(dims[-1], num_classes)\n        self.apply(self._init_weights)\n    def _init_weights(self, m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.ones_(m.weight)\n            nn.init.zeros_(m.bias)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.stem(x)\n        x = self.stages(x)\n        x = self.global_pool(x)\n        x = torch.flatten(x, 1)\n        return self.head(x)\n",
      "outputs": []
    },
    {
      "id": "aea68a44",
      "cell_type": "markdown",
      "source": "## Training and evaluation functions\n\nHere we implement the training loop, validation routine, history plotting and evaluation helpers.\nPlots of the training history (loss and accuracy) use the same blue palette.",
      "metadata": {}
    },
    {
      "id": "8dfc8298",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def train_one_epoch(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, optimizer: torch.optim.Optimizer, device: torch.device) -> tuple:\n    model.train()\n    running_loss = 0.0\n    running_corrects = 0\n    total = 0\n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        _, preds = torch.max(outputs, 1)\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels)\n        total += inputs.size(0)\n    epoch_loss = running_loss / total\n    epoch_acc = running_corrects.double().item() / total\n    return epoch_loss, epoch_acc\n\ndef validate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, device: torch.device) -> tuple:\n    model.eval()\n    running_loss = 0.0\n    running_corrects = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            _, preds = torch.max(outputs, 1)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels)\n            total += inputs.size(0)\n    epoch_loss = running_loss / total\n    epoch_acc = running_corrects.double().item() / total\n    return epoch_loss, epoch_acc\n\ndef train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, cfg: Config, device: torch.device):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = AdamW(model.parameters(), lr=cfg.LEARNING_RATE, weight_decay=cfg.WEIGHT_DECAY)\n    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.EPOCHS)\n    best_acc = 0.0\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n    for epoch in range(cfg.EPOCHS):\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n        scheduler.step()\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        print(f'Epoch {epoch+1}/{cfg.EPOCHS}  |  Train Loss: {train_loss:.4f}  Acc: {train_acc:.4f}  |  Val Loss: {val_loss:.4f}  Acc: {val_acc:.4f}')\n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save(model.state_dict(), 'best_econvnext_model.pth')\n    print(f'Best validation accuracy: {best_acc:.4f}')\n    return history\n\ndef plot_training_history(history: dict) -> None:\n    epochs_range = range(1, len(history['train_loss']) + 1)\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    axes[0].plot(epochs_range, history['train_loss'], label='Train Loss', color=sns.color_palette('Blues')[4])\n    axes[0].plot(epochs_range, history['val_loss'], label='Val Loss', color=sns.color_palette('Blues')[2])\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Loss Curves')\n    axes[0].legend()\n    axes[1].plot(epochs_range, history['train_acc'], label='Train Acc', color=sns.color_palette('Blues')[4])\n    axes[1].plot(epochs_range, history['val_acc'], label='Val Acc', color=sns.color_palette('Blues')[2])\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].set_title('Accuracy Curves')\n    axes[1].legend()\n    plt.show()\n\ndef evaluate_model(model: nn.Module, dataloader: DataLoader, class_names: list, device: torch.device) -> None:\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n    print(classification_report(all_labels, all_preds, target_names=class_names))\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n\ndef load_model_for_inference(weight_path: str, num_classes: int, variant: str, device: torch.device) -> nn.Module:\n    model = EConvNeXt(num_classes=num_classes, variant=variant)\n    model.load_state_dict(torch.load(weight_path, map_location=device))\n    model.to(device)\n    model.eval()\n    return model\n\ndef predict_image(model: nn.Module, image_path: str, cfg: Config, class_names: list, device: torch.device, topk: int = 5) -> None:\n    transform = transforms.Compose([\n        transforms.Resize(cfg.IMAGE_SIZE + 32),\n        transforms.CenterCrop(cfg.IMAGE_SIZE),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    img = Image.open(image_path).convert('RGB')\n    input_tensor = transform(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        outputs = model(input_tensor)\n        probs = torch.softmax(outputs[0], dim=0)\n        top_probs, top_idxs = torch.topk(probs, k=min(topk, len(probs)))\n    for i in range(len(top_probs)):\n        print(f'{class_names[top_idxs[i].item()]}: {top_probs[i].item():.4f}')\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title('Input Image')\n    plt.show()\n",
      "outputs": []
    },
    {
      "id": "0c14077b",
      "cell_type": "markdown",
      "source": "## Putting it all together\n\nIn this final section you can run the full pipeline. Provide the path to your dataset, split the data if necessary, perform EDA, train the model and evaluate it. The example below demonstrates the typical order of operations. Replace `'path/to/your/dataset'` with the actual location of your image data.",
      "metadata": {}
    },
    {
      "id": "7bed83d3",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# Specify your dataset root here. It may contain class sub\u2011folders directly or train/val/test splits.\ndataset_root = config.DATA_DIR  # update this variable before running\n\n# Step 1: split the dataset (if required)\nsplit_dataset(dataset_root, train_ratio=0.70, val_ratio=0.15, test_ratio=0.15, seed=SEED)\n\n# Step 2: run EDA on the training set (or on the entire dataset)\ntrain_path = os.path.join(dataset_root, 'train')\ndf_summary = run_eda(train_path)\n\n# Step 3: load data for training and validation\ntrain_loader, val_loader, class_names = load_data(config)\nconfig.NUM_CLASSES = len(class_names)\n\n# Step 4: initialise model and train\nmodel = EConvNeXt(num_classes=config.NUM_CLASSES, variant=config.MODEL_VARIANT).to(device)\nhistory = train_model(model, train_loader, val_loader, config, device)\nplot_training_history(history)\n\n# Step 5: evaluate the best saved model on the validation set\nbest_model = load_model_for_inference('best_econvnext_model.pth', num_classes=config.NUM_CLASSES, variant=config.MODEL_VARIANT, device=device)\nevaluate_model(best_model, val_loader, class_names, device)\n\n# Step 6: optional inference on a single image\n# image_path = '/path/to/your/image.jpg'\n# predict_image(best_model, image_path, config, class_names, device, topk=5)\n",
      "outputs": []
    }
  ]
}