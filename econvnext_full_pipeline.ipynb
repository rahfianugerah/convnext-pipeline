{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "21af0347",
      "metadata": {},
      "source": [
        "# E\u2011ConvNeXt Image Classification Pipeline\n",
        "\n",
        "This notebook presents a unified image classification workflow built around the E\u2011ConvNeXt architecture. It brings together several pieces of functionality from separate notebooks and scripts into one cohesive pipeline:\n",
        "\n",
        "* **Automated data splitting:** given a root directory of images, the notebook can automatically split the data into training, validation and test sets (70/15/15 split by default).\n",
        "* **Exploratory Data Analysis (EDA):** before training, the notebook scans the dataset and visualises key characteristics such as class distribution, image dimensions, aspect ratios and brightness. Example images are also shown.\n",
        "* **E\u2011ConvNeXt model definition and training:** the core model, training loop and evaluation functions are adapted from the *E\u2011ConvNeXt Notebook Guide\u2011FULLPIPELINE* to provide a configurable classification backbone. Training history is plotted using a consistent blue\u2011gradient colour palette.\n",
        "\n",
        "To use this notebook with your own data, set the `dataset_root` variable in the next code cell to point to your image folder. The folder can either contain class sub\u2011directories directly or be organised into `train`, `val`/`validation` and `test` folders. Once the dataset is split and loaded, the notebook will run EDA, train the E\u2011ConvNeXt model and provide evaluation metrics and visualisations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9ad48ab8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": "# Install required libraries (uncomment if running locally)\n# !pip install torch torchvision timm numpy pandas matplotlib seaborn scikit-learn\n\nimport os\nimport shutil\nimport random\nimport math\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n\nfrom torchvision import datasets, transforms\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Set random seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# Use a blue gradient palette for all plots\nsns.set_palette('Blues')"
    },
    {
      "cell_type": "markdown",
      "id": "97c8c659",
      "metadata": {},
      "source": [
        "## Automated dataset splitting\n",
        "\n",
        "The following function inspects a dataset directory and organises it into `train`, `val` and `test` sub\u2011folders on disk.\n",
        "\n",
        "* If the root contains only class sub\u2011folders, a 70/15/15 split is created.\n",
        "* If only `train` and `val` directories exist, their contents are combined and re\u2011split into `train`, `val` and `test`.\n",
        "* If `train` and `test` directories exist, they are left untouched (no validation split).\n",
        "* If `train`, `val` and `test` are present, no splitting is performed.\n",
        "\n",
        "You can adjust the `train_ratio`, `val_ratio` and `test_ratio` arguments to change the default 70/15/15 proportions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "34d2da44",
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_dataset(base_dir: str, train_ratio: float = 0.70, val_ratio: float = 0.15, test_ratio: float = 0.15, seed: int = 42) -> None:\n",
        "    \"\"\"\n",
        "    Split a dataset of images on disk. The function operates in four modes based on folder structure:\n",
        "\n",
        "    1. When the root contains only class sub\u2011folders (no `train`/`val`/`test`), a new `train`/`val`/`test` split is created.\n",
        "    2. When `train` and `val` folders exist, their contents are combined and re\u2011split into `train`/`val`/`test`.\n",
        "    3. When `train` and `test` exist, they are left untouched and no validation set is created.\n",
        "    4. When `train`, `val` and `test` exist, the directory structure is assumed correct and nothing is changed.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    base_dir : str\n",
        "        Path to the dataset root.\n",
        "    train_ratio : float\n",
        "        Proportion of samples to allocate to the training set when performing a new split.\n",
        "    val_ratio : float\n",
        "        Proportion of samples to allocate to the validation set when performing a new split.\n",
        "    test_ratio : float\n",
        "        Proportion of samples to allocate to the test set when performing a new split.\n",
        "    seed : int\n",
        "        Random seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    root = os.path.abspath(base_dir)\n",
        "    train_dir = os.path.join(root, 'train')\n",
        "    val_dir = os.path.join(root, 'val')\n",
        "    test_dir = os.path.join(root, 'test')\n",
        "\n",
        "    def _ensure_empty(dir_path: str) -> None:\n",
        "        if not os.path.exists(dir_path):\n",
        "            os.makedirs(dir_path)\n",
        "        else:\n",
        "            for item in os.listdir(dir_path):\n",
        "                shutil.rmtree(os.path.join(dir_path, item))\n",
        "\n",
        "    has_train = os.path.isdir(train_dir)\n",
        "    has_val = os.path.isdir(val_dir)\n",
        "    has_test = os.path.isdir(test_dir)\n",
        "\n",
        "    # Case 1: single folder of class sub\u2011directories\n",
        "    if not has_train and not has_val and not has_test:\n",
        "        print('No train/val/test directories detected. Creating a 70/15/15 split.')\n",
        "        classes = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n",
        "        _ensure_empty(train_dir)\n",
        "        _ensure_empty(val_dir)\n",
        "        _ensure_empty(test_dir)\n",
        "        for cls in classes:\n",
        "            src_dir = os.path.join(root, cls)\n",
        "            images = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n",
        "            random.shuffle(images)\n",
        "            n_total = len(images)\n",
        "            n_train = int(n_total * train_ratio)\n",
        "            n_val = int(n_total * val_ratio)\n",
        "            splits = {\n",
        "                'train': images[:n_train],\n",
        "                'val': images[n_train:n_train + n_val],\n",
        "                'test': images[n_train + n_val:]\n",
        "            }\n",
        "            for split_name, files in splits.items():\n",
        "                target_dir = os.path.join(root, split_name, cls)\n",
        "                os.makedirs(target_dir, exist_ok=True)\n",
        "                for fname in files:\n",
        "                    shutil.copy2(os.path.join(src_dir, fname), os.path.join(target_dir, fname))\n",
        "    # Case 2: train and val only \u2192 combine and re\u2011split\n",
        "    elif has_train and has_val and not has_test:\n",
        "        print('Found train and val directories only. Combining and re\u2011splitting into train/val/test.')\n",
        "        combined = {}\n",
        "        for split_dir in [train_dir, val_dir]:\n",
        "            for cls in os.listdir(split_dir):\n",
        "                cls_dir = os.path.join(split_dir, cls)\n",
        "                combined.setdefault(cls, []).extend([\n",
        "                    os.path.join(cls_dir, f) for f in os.listdir(cls_dir)\n",
        "                    if os.path.isfile(os.path.join(cls_dir, f))\n",
        "                ])\n",
        "        _ensure_empty(train_dir)\n",
        "        _ensure_empty(val_dir)\n",
        "        _ensure_empty(test_dir)\n",
        "        for cls, files in combined.items():\n",
        "            random.shuffle(files)\n",
        "            n_total = len(files)\n",
        "            n_train = int(n_total * train_ratio)\n",
        "            n_val = int(n_total * val_ratio)\n",
        "            splits = {\n",
        "                'train': files[:n_train],\n",
        "                'val': files[n_train:n_train + n_val],\n",
        "                'test': files[n_train + n_val:]\n",
        "            }\n",
        "            for split_name, filepaths in splits.items():\n",
        "                target_dir = os.path.join(root, split_name, cls)\n",
        "                os.makedirs(target_dir, exist_ok=True)\n",
        "                for src_path in filepaths:\n",
        "                    shutil.copy2(src_path, os.path.join(target_dir, os.path.basename(src_path)))\n",
        "    # Case 3: train and test only \u2192 do nothing\n",
        "    elif has_train and not has_val and has_test:\n",
        "        print('Found train and test directories only. Leaving as\u2011is. No validation set will be created.')\n",
        "    # Case 4: train, val and test exist \u2192 do nothing\n",
        "    elif has_train and has_val and has_test:\n",
        "        print('Found train, val, and test directories. No splitting needed.')\n",
        "    else:\n",
        "        raise RuntimeError(f'Unrecognised layout under {root}. Expected combinations of train/val/test.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74ededdf",
      "metadata": {},
      "source": [
        "### Configuration and data loading\n",
        "\n",
        "The configuration below allows you to customise the dataset location, image resolution, model variant and training hyperparameters. After splitting the dataset, the `load_data` function loads the images into `torchvision.datasets.ImageFolder` datasets and constructs PyTorch dataloaders for the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e0a738fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Editable configuration\n",
        "@dataclass\n",
        "class Config:\n",
        "    DATA_DIR: str = 'dataset/'  # change this to your dataset root\n",
        "    NUM_CLASSES: int = 10\n",
        "    MODEL_VARIANT: str = 'tiny'            # choose from 'mini', 'tiny', 'small'\n",
        "    IMAGE_SIZE: int = 224\n",
        "    BATCH_SIZE: int = 32\n",
        "    EPOCHS: int = 10\n",
        "    LEARNING_RATE: float = 2.5e-4\n",
        "    WEIGHT_DECAY: float = 0.05\n",
        "    # augmentation settings\n",
        "    RAND_AUGMENT_N: int = 9\n",
        "    RAND_AUGMENT_M: float = 0.5\n",
        "    MIXUP_ALPHA: float = None\n",
        "    CUTMIX_ALPHA: float = None\n",
        "\n",
        "config = Config()\n",
        "\n",
        "def get_transforms(cfg: Config):\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize(cfg.IMAGE_SIZE + 32),\n",
        "        transforms.RandomCrop(cfg.IMAGE_SIZE),\n",
        "        transforms.RandAugment(num_ops=cfg.RAND_AUGMENT_N, magnitude=int(cfg.RAND_AUGMENT_M * 10)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    val_transforms = transforms.Compose([\n",
        "        transforms.Resize(cfg.IMAGE_SIZE + 32),\n",
        "        transforms.CenterCrop(cfg.IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    return train_transforms, val_transforms\n",
        "\n",
        "def load_data(cfg: Config):\n",
        "    train_transforms, val_transforms = get_transforms(cfg)\n",
        "    train_dir = os.path.join(cfg.DATA_DIR, 'train')\n",
        "    val_dir = os.path.join(cfg.DATA_DIR, 'val')\n",
        "    train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
        "    val_dataset = datasets.ImageFolder(val_dir, transform=val_transforms)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    return train_loader, val_loader, train_dataset.classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b873275",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "\n",
        "The following helpers perform a lightweight scan of an image dataset and produce visual summaries. Statistics such as image width, height, aspect ratio, file size, brightness and colourfulness are collected. Plotting functions then visualise class distribution, dimension distributions, aspect ratios and other metrics. All charts use the same blue\u2011gradient palette for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3ed8cfc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def _colorfulness_fast(im: Image.Image) -> float:\n",
        "    arr = np.asarray(im.convert('RGB'), dtype=np.float32)\n",
        "    r, g, b = arr[..., 0], arr[..., 1], arr[..., 2]\n",
        "    rg = np.abs(r - g)\n",
        "    yb = np.abs(0.5 * (r + g) - b)\n",
        "    return float(math.sqrt(rg.var() + yb.var()) + 0.3 * math.sqrt(rg.mean()**2 + yb.mean()**2))\n",
        "\n",
        "def _brightness(im: Image.Image) -> float:\n",
        "    return float(np.asarray(im.convert('L'), dtype=np.float32).mean())\n",
        "\n",
        "def scan_image_dataset(data_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Perform a scan over all images under `data_path` and return a DataFrame summarising their properties.\n",
        "\n",
        "    The scan looks recursively for files with typical image extensions. For each image it records\n",
        "    the label (sub\u2011directory name), optional split (train/val/test if present), dimensions, aspect ratio,\n",
        "    file size (kilobytes), brightness and colourfulness.\n",
        "    \"\"\"\n",
        "    img_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}\n",
        "    records = []\n",
        "    for root_dir, _, files in os.walk(data_path):\n",
        "        for fname in files:\n",
        "            if not any(fname.lower().endswith(ext) for ext in img_exts):\n",
        "                continue\n",
        "            fpath = os.path.join(root_dir, fname)\n",
        "            try:\n",
        "                with Image.open(fpath) as im:\n",
        "                    w, h = im.size\n",
        "                    mode = im.mode\n",
        "                    brightness_value = _brightness(im)\n",
        "                    colorfulness_value = _colorfulness_fast(im)\n",
        "            except Exception:\n",
        "                continue\n",
        "            parts = os.path.normpath(fpath).split(os.sep)\n",
        "            label = parts[-2] if len(parts) >= 2 else ''\n",
        "            split = ''\n",
        "            if label.lower() in {'train', 'val', 'validation', 'test'}:\n",
        "                split = label.lower()\n",
        "                label = parts[-3] if len(parts) >= 3 else label\n",
        "            records.append({\n",
        "                'path': fpath,\n",
        "                'split': split,\n",
        "                'label': label,\n",
        "                'width': w,\n",
        "                'height': h,\n",
        "                'aspect': (w / h) if h > 0 else float('nan'),\n",
        "                'file_size_kb': os.path.getsize(fpath) / 1024.0,\n",
        "                'brightness': brightness_value,\n",
        "                'colorfulness': colorfulness_value,\n",
        "                'mode': mode\n",
        "            })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "def plot_class_distribution(df: pd.DataFrame) -> None:\n",
        "    counts = df['label'].value_counts().sort_values(ascending=False)\n",
        "    plt.figure(figsize=(max(6, 0.35 * len(counts) + 4), 4))\n",
        "    sns.barplot(x=counts.index, y=counts.values)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.title('Class Distribution')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Number of images')\n",
        "    plt.show()\n",
        "\n",
        "def plot_histogram(series: np.ndarray, bins: int, title: str, xlabel: str) -> None:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.hist(series, bins=bins, color=sns.color_palette('Blues')[3])\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter(x: np.ndarray, y: np.ndarray, title: str, xlabel: str, ylabel: str) -> None:\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.scatter(x, y, alpha=0.6, color=sns.color_palette('Blues')[4])\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()\n",
        "\n",
        "def show_random_images(df: pd.DataFrame, n: int = 9) -> None:\n",
        "    n = min(n, len(df))\n",
        "    indices = np.random.choice(len(df), size=n, replace=False)\n",
        "    sample_paths = df.iloc[indices]['path'].tolist()\n",
        "    grid_size = int(math.sqrt(n))\n",
        "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size * 3, grid_size * 3))\n",
        "    for idx, path in enumerate(sample_paths):\n",
        "        row = idx // grid_size\n",
        "        col = idx % grid_size\n",
        "        ax = axes[row, col]\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(os.path.basename(os.path.dirname(path)), fontsize=8)\n",
        "        except Exception:\n",
        "            ax.axis('off')\n",
        "            continue\n",
        "        ax.axis('off')\n",
        "    plt.suptitle('Random Image Samples')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def run_eda(data_path: str) -> pd.DataFrame:\n",
        "    df = scan_image_dataset(data_path)\n",
        "    if df.empty:\n",
        "        print(f'No images found under: {data_path}')\n",
        "        return df\n",
        "    plot_class_distribution(df)\n",
        "    plot_histogram(df['width'].values, bins=40, title='Image Width Distribution', xlabel='Width (pixels)')\n",
        "    plot_histogram(df['height'].values, bins=40, title='Image Height Distribution', xlabel='Height (pixels)')\n",
        "    aspects = df['aspect'].dropna().values\n",
        "    plot_histogram(aspects, bins=40, title='Aspect Ratio Distribution', xlabel='Aspect ratio (W/H)')\n",
        "    plot_histogram(df['file_size_kb'].values, bins=40, title='File Size Distribution', xlabel='File size (KB)')\n",
        "    plot_histogram(df['brightness'].values, bins=40, title='Brightness Distribution', xlabel='Average grayscale value (0\u2013255)')\n",
        "    plot_scatter(df['width'].values, df['height'].values, title='Width vs Height', xlabel='Width (pixels)', ylabel='Height (pixels)')\n",
        "    show_random_images(df, n=9)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e2fd994",
      "metadata": {},
      "source": [
        "## E\u2011ConvNeXt model architecture\n",
        "\n",
        "The following cells define the E\u2011ConvNeXt architecture adapted from the reference implementation.\n",
        "It comprises an Effective Squeeze\u2011and\u2011Excitation (ESE) block, a modified ConvNeXt block with Batch Normalisation, a Cross\u2011Stage Partial (CSP) stage and the overall model class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e148211c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Effective Squeeze\u2011and\u2011Excitation block\n",
        "class ESEBlock(nn.Module):\n",
        "    def __init__(self, channels: int, reduction: int = 4):\n",
        "        super().__init__()\n",
        "        reduced_channels = max(1, channels // reduction)\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=True),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(reduced_channels, channels, kernel_size=1, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        s = self.squeeze(x)\n",
        "        e = self.excitation(s)\n",
        "        return x * e\n",
        "\n",
        "# E\u2011ConvNeXt block\n",
        "class EConvNeXtBlock(nn.Module):\n",
        "    def __init__(self, dim: int, expansion_factor: float = 4.0):\n",
        "        super().__init__()\n",
        "        expanded_dim = int(dim * expansion_factor)\n",
        "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
        "        self.bn1 = nn.BatchNorm2d(dim)\n",
        "        self.pwconv1 = nn.Conv2d(dim, expanded_dim, kernel_size=1)\n",
        "        self.bn2 = nn.BatchNorm2d(expanded_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.pwconv2 = nn.Conv2d(expanded_dim, dim, kernel_size=1)\n",
        "        self.bn3 = nn.BatchNorm2d(dim)\n",
        "        self.ese = ESEBlock(dim)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        residual = x\n",
        "        x = self.dwconv(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pwconv2(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.ese(x)\n",
        "        return x + residual\n",
        "\n",
        "# CSP stage\n",
        "class CSPStage(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, num_blocks: int, ch_mid: int = None):\n",
        "        super().__init__()\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        if ch_mid is None:\n",
        "            ch_mid = (in_channels + out_channels) // 2\n",
        "        self.split_conv0 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, ch_mid, kernel_size=1),\n",
        "            nn.BatchNorm2d(ch_mid),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        self.split_conv1 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, ch_mid, kernel_size=1),\n",
        "            nn.BatchNorm2d(ch_mid),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        self.blocks = nn.Sequential(*[EConvNeXtBlock(ch_mid) for _ in range(num_blocks)])\n",
        "        self.after_blocks = nn.Sequential(\n",
        "            nn.Conv2d(ch_mid, ch_mid, kernel_size=1),\n",
        "            nn.BatchNorm2d(ch_mid),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        self.merge_conv = nn.Sequential(\n",
        "            nn.Conv2d(ch_mid * 2, out_channels, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.downsample(x)\n",
        "        y1 = self.split_conv0(x)\n",
        "        y2 = self.split_conv1(x)\n",
        "        y2 = self.blocks(y2)\n",
        "        y2 = self.after_blocks(y2)\n",
        "        merged = torch.cat([y1, y2], dim=1)\n",
        "        return self.merge_conv(merged)\n",
        "\n",
        "def get_variant_config(variant: str):\n",
        "    variant = variant.lower()\n",
        "    if variant == 'mini':\n",
        "        dims = [48, 96, 192, 384]\n",
        "        depths = [3, 3, 9, 3]\n",
        "    elif variant == 'tiny':\n",
        "        dims = [64, 128, 256, 512]\n",
        "        depths = [3, 3, 9, 3]\n",
        "    elif variant == 'small':\n",
        "        dims = [80, 160, 320, 640]\n",
        "        depths = [3, 3, 27, 3]\n",
        "    else:\n",
        "        raise ValueError(f'Unknown variant: {variant}')\n",
        "    return dims, depths\n",
        "\n",
        "class EConvNeXt(nn.Module):\n",
        "    def __init__(self, num_classes: int = 1000, variant: str = 'tiny'):\n",
        "        super().__init__()\n",
        "        dims, depths = get_variant_config(variant)\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(32, dims[0], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(dims[0]),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        in_channels = dims[0]\n",
        "        stages = []\n",
        "        for out_channels, depth in zip(dims, depths):\n",
        "            stages.append(CSPStage(in_channels, out_channels, num_blocks=depth))\n",
        "            in_channels = out_channels\n",
        "        self.stages = nn.Sequential(*stages)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.head = nn.Linear(dims[-1], num_classes)\n",
        "        self.apply(self._init_weights)\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.stem(x)\n",
        "        x = self.stages(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.head(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aea68a44",
      "metadata": {},
      "source": [
        "## Training and evaluation functions\n",
        "\n",
        "Here we implement the training loop, validation routine, history plotting and evaluation helpers.\n",
        "Plots of the training history (loss and accuracy) use the same blue palette."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8dfc8298",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, optimizer: torch.optim.Optimizer, device: torch.device) -> tuple:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels)\n",
        "        total += inputs.size(0)\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = running_corrects.double().item() / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, device: torch.device) -> tuple:\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels)\n",
        "            total += inputs.size(0)\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = running_corrects.double().item() / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, cfg: Config, device: torch.device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=cfg.LEARNING_RATE, weight_decay=cfg.WEIGHT_DECAY)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.EPOCHS)\n",
        "    best_acc = 0.0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    for epoch in range(cfg.EPOCHS):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        print(f'Epoch {epoch+1}/{cfg.EPOCHS}  |  Train Loss: {train_loss:.4f}  Acc: {train_acc:.4f}  |  Val Loss: {val_loss:.4f}  Acc: {val_acc:.4f}')\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_econvnext_model.pth')\n",
        "    print(f'Best validation accuracy: {best_acc:.4f}')\n",
        "    return history\n",
        "\n",
        "def plot_training_history(history: dict) -> None:\n",
        "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    axes[0].plot(epochs_range, history['train_loss'], label='Train Loss', color=sns.color_palette('Blues')[4])\n",
        "    axes[0].plot(epochs_range, history['val_loss'], label='Val Loss', color=sns.color_palette('Blues')[2])\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Loss Curves')\n",
        "    axes[0].legend()\n",
        "    axes[1].plot(epochs_range, history['train_acc'], label='Train Acc', color=sns.color_palette('Blues')[4])\n",
        "    axes[1].plot(epochs_range, history['val_acc'], label='Val Acc', color=sns.color_palette('Blues')[2])\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].set_title('Accuracy Curves')\n",
        "    axes[1].legend()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_model(model: nn.Module, dataloader: DataLoader, class_names: list, device: torch.device) -> None:\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def load_model_for_inference(weight_path: str, num_classes: int, variant: str, device: torch.device) -> nn.Module:\n",
        "    model = EConvNeXt(num_classes=num_classes, variant=variant)\n",
        "    model.load_state_dict(torch.load(weight_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def predict_image(model: nn.Module, image_path: str, cfg: Config, class_names: list, device: torch.device, topk: int = 5) -> None:\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(cfg.IMAGE_SIZE + 32),\n",
        "        transforms.CenterCrop(cfg.IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        probs = torch.softmax(outputs[0], dim=0)\n",
        "        top_probs, top_idxs = torch.topk(probs, k=min(topk, len(probs)))\n",
        "    for i in range(len(top_probs)):\n",
        "        print(f'{class_names[top_idxs[i].item()]}: {top_probs[i].item():.4f}')\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Input Image')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "boosting_intro",
      "metadata": {},
      "source": [
        "## Boosting Technique: Snapshot Ensemble\n",
        "\n",
        "To improve model accuracy while considering the computational constraints of a laptop with i7 CPU and NVIDIA RTX 3050 GPU, we implement a **Snapshot Ensemble** boosting technique. This approach:\n",
        "\n",
        "* **Saves multiple model snapshots** during training at specific epochs (when the cyclic learning rate reaches its minimum)\n",
        "* **Combines predictions** from these snapshots through averaging to create a stronger ensemble\n",
        "* **Optimized for laptop hardware**: Uses only a single training run, avoiding the computational cost of training multiple separate models\n",
        "* **Balanced performance**: Provides accuracy improvements with minimal impact on training time and moderate inference overhead\n",
        "\n",
        "The snapshot ensemble leverages the fact that models trained with cyclic learning rates converge to different local minima, creating diversity in the ensemble without the need for multiple full training runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "boosting_implementation",
      "metadata": {},
      "outputs": [],
      "source": "import time\n\ndef train_model_with_snapshots(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, cfg: Config, device: torch.device, n_snapshots: int = 3):\n    \"\"\"\n    Train model with snapshot ensemble using cyclic learning rate.\n    Saves model snapshots at local minima for ensemble boosting.\n    \n    Parameters\n    ----------\n    model : nn.Module\n        The model to train\n    train_loader : DataLoader\n        Training data loader\n    val_loader : DataLoader\n        Validation data loader\n    cfg : Config\n        Configuration object\n    device : torch.device\n        Device to train on\n    n_snapshots : int\n        Number of snapshots to save (default: 3 for RTX 3050 memory efficiency)\n    \"\"\"\n    criterion = nn.CrossEntropyLoss()\n    optimizer = AdamW(model.parameters(), lr=cfg.LEARNING_RATE, weight_decay=cfg.WEIGHT_DECAY)\n    \n    # Cyclic learning rate scheduler for snapshot ensemble\n    # T_0 determines cycle length - we use epochs/n_snapshots to get n_snapshots cycles\n    cycle_length = max(1, cfg.EPOCHS // n_snapshots)\n    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=cycle_length, T_mult=1)\n    \n    best_acc = 0.0\n    snapshot_paths = []\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n    training_start_time = time.time()\n    \n    for epoch in range(cfg.EPOCHS):\n        epoch_start = time.time()\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n        scheduler.step()\n        epoch_time = time.time() - epoch_start\n        \n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        \n        print(f'Epoch {epoch+1}/{cfg.EPOCHS}  |  Train Loss: {train_loss:.4f}  Acc: {train_acc:.4f}  |  '\n              f'Val Loss: {val_loss:.4f}  Acc: {val_acc:.4f}  |  Time: {epoch_time:.2f}s')\n        \n        # Save snapshot at the end of each cycle (when LR is at minimum)\n        if (epoch + 1) % cycle_length == 0 and len(snapshot_paths) < n_snapshots:\n            snapshot_path = f'snapshot_model_{len(snapshot_paths)+1}.pth'\n            torch.save(model.state_dict(), snapshot_path)\n            snapshot_paths.append(snapshot_path)\n            print(f'  \u2192 Saved snapshot {len(snapshot_paths)}/{n_snapshots}: {snapshot_path}')\n        \n        # Also keep track of best single model\n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save(model.state_dict(), 'best_econvnext_model_boosted.pth')\n    \n    total_training_time = time.time() - training_start_time\n    print(f'\\nTraining completed in {total_training_time:.2f}s ({total_training_time/60:.2f} minutes)')\n    print(f'Best single model validation accuracy: {best_acc:.4f}')\n    print(f'Saved {len(snapshot_paths)} snapshots for ensemble')\n    \n    return history, snapshot_paths, total_training_time\n\ndef ensemble_predict(models: list, dataloader: DataLoader, device: torch.device) -> tuple:\n    \"\"\"\n    Perform ensemble prediction by averaging logits from multiple models.\n    \n    Parameters\n    ----------\n    models : list\n        List of trained models\n    dataloader : DataLoader\n        Data loader for inference\n    device : torch.device\n        Device to run inference on\n    \n    Returns\n    -------\n    tuple\n        (all_predictions, all_labels) as numpy arrays\n    \"\"\"\n    for model in models:\n        model.eval()\n    \n    all_preds = []\n    all_labels = []\n    \n    inference_start = time.time()\n    \n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs = inputs.to(device)\n            \n            # Collect predictions from all models\n            ensemble_logits = []\n            for model in models:\n                logits = model(inputs)\n                ensemble_logits.append(logits)\n            \n            # Average the logits\n            avg_logits = torch.stack(ensemble_logits).mean(dim=0)\n            _, preds = torch.max(avg_logits, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n    \n    inference_time = time.time() - inference_start\n    print(f'Ensemble inference time: {inference_time:.2f}s')\n    print(f'Average time per batch: {inference_time/len(dataloader):.4f}s')\n    \n    return np.array(all_preds), np.array(all_labels)\n\ndef evaluate_ensemble(snapshot_paths: list, num_classes: int, variant: str, dataloader: DataLoader, class_names: list, device: torch.device) -> float:\n    \"\"\"\n    Evaluate the snapshot ensemble on a dataset.\n    \n    Parameters\n    ----------\n    snapshot_paths : list\n        List of paths to snapshot model weights\n    num_classes : int\n        Number of output classes\n    variant : str\n        Model variant ('mini', 'tiny', 'small')\n    dataloader : DataLoader\n        Data loader for evaluation\n    class_names : list\n        List of class names\n    device : torch.device\n        Device to run evaluation on\n    \n    Returns\n    -------\n    float\n        Ensemble accuracy\n    \"\"\"\n    print(f'\\nLoading {len(snapshot_paths)} snapshot models for ensemble...')\n    models = []\n    for path in snapshot_paths:\n        model = EConvNeXt(num_classes=num_classes, variant=variant)\n        model.load_state_dict(torch.load(path, map_location=device))\n        model.to(device)\n        models.append(model)\n    \n    print('\\nEnsemble Prediction Results:')\n    all_preds, all_labels = ensemble_predict(models, dataloader, device)\n    \n    print('\\nEnsemble Classification Report:')\n    print(classification_report(all_labels, all_preds, target_names=class_names))\n    \n    # Plot confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix - Snapshot Ensemble')\n    plt.show()\n    \n    # Calculate and display ensemble accuracy\n    ensemble_acc = (all_preds == all_labels).mean()\n    print(f'\\nSnapshot Ensemble Accuracy: {ensemble_acc:.4f}')\n    \n    return ensemble_acc\n\ndef compare_baseline_vs_ensemble(baseline_acc: float, ensemble_acc: float, training_time: float) -> None:\n    \"\"\"\n    Visualize comparison between baseline and ensemble models.\n    \n    Parameters\n    ----------\n    baseline_acc : float\n        Baseline model accuracy\n    ensemble_acc : float\n        Ensemble model accuracy\n    training_time : float\n        Total training time in seconds\n    \"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Accuracy comparison\n    models = ['Baseline\\n(Single Model)', 'Snapshot\\nEnsemble']\n    accuracies = [baseline_acc, ensemble_acc]\n    colors = [sns.color_palette('Blues')[3], sns.color_palette('Blues')[5]]\n    \n    axes[0].bar(models, accuracies, color=colors, alpha=0.8)\n    axes[0].set_ylabel('Validation Accuracy')\n    axes[0].set_title('Model Performance Comparison')\n    axes[0].set_ylim([min(accuracies) - 0.05, 1.0])\n    \n    # Add value labels on bars\n    for i, (model, acc) in enumerate(zip(models, accuracies)):\n        axes[0].text(i, acc + 0.01, f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n    \n    # Improvement metrics\n    improvement = ensemble_acc - baseline_acc\n    improvement_pct = (improvement / baseline_acc) * 100\n    \n    separator = '=' * 30\n    metrics_text = (\n        f'Performance Summary\\n'\n        f'{separator}\\n'\n        f'Baseline Accuracy: {baseline_acc:.4f}\\n'\n        f'Ensemble Accuracy: {ensemble_acc:.4f}\\n'\n        f'Improvement: {improvement:.4f} ({improvement_pct:+.2f}%)\\n'\n        f'\\n'\n        f'Training Time: {training_time/60:.2f} min\\n'\n        f'\\n'\n        f'Hardware: i7 CPU + RTX 3050 GPU\\n'\n        f'Technique: Snapshot Ensemble\\n'\n        f'Memory Efficient: \u2713'\n    )\n    \n    axes[1].text(0.1, 0.5, metrics_text, transform=axes[1].transAxes,\n                fontsize=11, verticalalignment='center',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3),\n                family='monospace')\n    axes[1].axis('off')\n    axes[1].set_title('Performance Metrics')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print('\\n' + '='*60)\n    print('BOOSTING RESULTS SUMMARY')\n    print('='*60)\n    print(f'Baseline Model Accuracy:    {baseline_acc:.4f}')\n    print(f'Snapshot Ensemble Accuracy: {ensemble_acc:.4f}')\n    print(f'Accuracy Improvement:       {improvement:.4f} ({improvement_pct:+.2f}%)')\n    print(f'Total Training Time:        {training_time/60:.2f} minutes')\n    print('='*60)\n"
    },
    {
      "cell_type": "markdown",
      "id": "pipeline_with_boosting",
      "metadata": {},
      "source": [
        "## Putting it all together: Baseline + Boosting\n",
        "\n",
        "In this final section you can run the full pipeline including the snapshot ensemble boosting. The workflow consists of:\n",
        "\n",
        "1. **Data preparation**: Split and load the dataset\n",
        "2. **Exploratory Data Analysis**: Visualize dataset characteristics\n",
        "3. **Baseline model training**: Train a single E\u2011ConvNeXt model\n",
        "4. **Baseline evaluation**: Evaluate the single model performance\n",
        "5. **Boosted model training**: Train with snapshot ensemble (cyclic LR)\n",
        "6. **Ensemble evaluation**: Evaluate the snapshot ensemble\n",
        "7. **Performance comparison**: Compare baseline vs boosted results\n",
        "\n",
        "Replace `'path/to/your/dataset'` with the actual location of your image data. The boosting technique is optimized for laptop hardware (i7 + RTX 3050) and provides a good balance between accuracy improvement and computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "main_pipeline_with_boosting",
      "metadata": {},
      "outputs": [],
      "source": "# Specify your dataset root here. It may contain class sub\u2011folders directly or train/val/test splits.\ndataset_root = config.DATA_DIR  # update this variable before running\n\n# Step 1: split the dataset (if required)\nsplit_dataset(dataset_root, train_ratio=0.70, val_ratio=0.15, test_ratio=0.15, seed=SEED)\n\n# Step 2: run EDA on the training set (or on the entire dataset)\ntrain_path = os.path.join(dataset_root, 'train')\ndf_summary = run_eda(train_path)\n\n# Step 3: load data for training and validation\ntrain_loader, val_loader, class_names = load_data(config)\nconfig.NUM_CLASSES = len(class_names)\n\nprint('\\n' + '='*80)\nprint('BASELINE MODEL TRAINING')\nprint('='*80)\n\n# Step 4: initialise and train baseline model (standard training)\nbaseline_model = EConvNeXt(num_classes=config.NUM_CLASSES, variant=config.MODEL_VARIANT).to(device)\nbaseline_history = train_model(baseline_model, train_loader, val_loader, config, device)\nplot_training_history(baseline_history)\n\n# Step 5: evaluate the baseline model on the validation set\nprint('\\nBaseline Model Evaluation:')\nbest_baseline_model = load_model_for_inference('best_econvnext_model.pth', num_classes=config.NUM_CLASSES, variant=config.MODEL_VARIANT, device=device)\nevaluate_model(best_baseline_model, val_loader, class_names, device)\n\n# Get baseline accuracy for comparison\nbaseline_val_acc = max(baseline_history['val_acc'])\n\nprint('\\n' + '='*80)\nprint('BOOSTED MODEL TRAINING (SNAPSHOT ENSEMBLE)')\nprint('='*80)\n\n# Step 6: train with snapshot ensemble boosting (optimized for laptop hardware)\nboosted_model = EConvNeXt(num_classes=config.NUM_CLASSES, variant=config.MODEL_VARIANT).to(device)\nboosted_history, snapshot_paths, training_time = train_model_with_snapshots(\n    boosted_model, train_loader, val_loader, config, device, n_snapshots=3\n)\nplot_training_history(boosted_history)\n\nprint('\\n' + '='*80)\nprint('ENSEMBLE EVALUATION')\nprint('='*80)\n\n# Step 7: evaluate the snapshot ensemble\nensemble_acc = evaluate_ensemble(\n    snapshot_paths, config.NUM_CLASSES, config.MODEL_VARIANT, \n    val_loader, class_names, device\n)\n\n# Step 8: compare baseline vs ensemble performance\ncompare_baseline_vs_ensemble(baseline_val_acc, ensemble_acc, training_time)\n\n# Step 9: optional inference on a single image using ensemble\n# Uncomment to test on a specific image:\n# image_path = '/path/to/your/image.jpg'\n# predict_image(best_baseline_model, image_path, config, class_names, device, topk=5)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gemastik",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}